#include <iostream>
#include <cmath>
#include <vector>
#include <iomanip>
#include <limits>

using namespace std;

const double epsilon = 0.01; // требуемая точность
const int M = 10000000;      // допустимое число итераций

// Функция
double func(double x1, double x2) {
    return pow((x1 - 2 * x2), 2) + pow((x2 - 3), 2);
}

// Первая производная по x1
double func_proizv_x1(double x1, double x2) {
    return 2 * (x1 - 2 * x2);
}

// Первая производная по x2
double func_proizv_x2(double x1, double x2) {
    return -4 * (x1 - 2 * x2) + 2 * (x2 - 3);
}

// Вторая производная по x1
double func_proizv2_x1(double x1, double x2) {
    return 2; // f''(x1) = 2
}

// Вторая производная по x2
double func_proizv2_x2(double x1, double x2) {
    return 10; // f''(x2) = 10
}

// Вторая производная по x1 и x2
double func_proizv2_x1x2(double x1, double x2) {
    return -4; // f''(x1, x2) = -4
}

// Метод Марквардта
int Markvarda(double& x1, double& x2) {
    int k = 0;
    double lambda = 1.0; // Начальное значение параметра регуляризации

    while (k < M) {
        // Вычисляем градиент
        double grad_x1 = func_proizv_x1(x1, x2);
        double grad_x2 = func_proizv_x2(x1, x2);

        // Проверка на точность
        if (sqrt(grad_x1 * grad_x1 + grad_x2 * grad_x2) < epsilon) {
            cout << "Метод Марквардта завершен: x* = ("
                << fixed << setprecision(2) << x1 << ", " << x2 << ")"
                << "\t" << "f(x1,x2)= " << func(x1, x2) << endl;
            cout << "Количество итераций: " << k << endl;
            return k;
        }

        // Вычисляем матрицу Гессе
        double H[2][2] = {
            {func_proizv2_x1(x1, x2), func_proizv2_x1x2(x1, x2)},
            {func_proizv2_x1x2(x1, x2), func_proizv2_x2(x1, x2)}
        };

        // Модифицируем матрицу Гессе
        H[0][0] += lambda;
        H[1][1] += lambda;

        // Вычисляем детерминант Гессиана
        double det = H[0][0] * H[1][1] - H[0][1] * H[1][0];

        // Проверка на нулевой детерминант
        if (fabs(det) < epsilon) {
            cout << "Детерминант Гессиана слишком мал, метод не может продолжаться." << endl;
            return -1; // Indicate failure
        }

        // Обратная матрица Гессе
        double H_inv[2][2] = {
            {H[1][1] / det, -H[0][1] / det},
            {-H[1][0] / det, H[0][0] / det}
        };

        // Вычисляем d = -H^(-1) * ∇f
        double d_x1 = -(H_inv[0][0] * grad_x1 + H_inv[0][1] * grad_x2);
        double d_x2 = -(H_inv[1][0] * grad_x1 + H_inv[1][1] * grad_x2);

        // Обновляем значения
        double next_x1 = x1 + d_x1;
        double next_x2 = x2 + d_x2;

        // Оцениваем изменение функции
        double delta_f = func(x1, x2) - func(next_x1, next_x2);

        if (delta_f > 0) {
            // Уменьшаем lambda, если улучшение есть
            x1 = next_x1;
            x2 = next_x2;
            lambda /= 2;
        }
        else {
            // Увеличиваем lambda, если ухудшение
            lambda *= 2;
        }

        k++; // Увеличиваем счетчик итераций
    }

    cout << "Достигнуто максимальное число итераций в методе Марквардта." << endl;
    return -1; // Indicate failure
}

// Метод Ньютона-Рафсона
int newton_raphson(double& x1, double& x2) {
    int k = 0;

    while (k < M) {
        // Градиент
        double grad_x1 = func_proizv_x1(x1, x2);
        double grad_x2 = func_proizv_x2(x1, x2);

        // Условие точности
        if (sqrt(grad_x1 * grad_x1 + grad_x2 * grad_x2) < epsilon) {
            cout << "Метод Ньютона-Рафсона завершен: x* = ("
                << fixed << setprecision(2) << x1 << ", " << x2 << ")"
                << "\t" << "f(x1,x2)= " << func(x1, x2) << endl;
            cout << "Количество итераций: " << k << endl;
            return k;
        }

        // Матрица Гессе
        double H[2][2] = {
            {func_proizv2_x1(x1, x2), func_proizv2_x1x2(x1, x2)},
            {func_proizv2_x1x2(x1, x2), func_proizv2_x2(x1, x2)}
        };

        // Определитель матрицы Гессе
        double det = H[0][0] * H[1][1] - H[0][1] * H[1][0];

        // Проверка на нулевой определитель
        if (fabs(det) < epsilon) {
            cout << "Отрицательный определитель" << endl;
            return -1;
        }

        // Обратная матрица Гессе
        double H_inv[2][2] = {
            {H[1][1] / det, -H[0][1] / det},
            {-H[1][0] / det, H[0][0] / det}
        };

        // d = -H^(-1) * ∇f
        double d_x1 = -(H_inv[0][0] * grad_x1 + H_inv[0][1] * grad_x2);
        double d_x2 = -(H_inv[1][0] * grad_x1 + H_inv[1][1] * grad_x2);

        // Определение шага λ (начальное значение)
        double lambda = 1.0;
        double x1_new = x1 + lambda * d_x1;
        double x2_new = x2 + lambda * d_x2;

        // Условие Вольфе для выбора подходящего λ
        while (func(x1_new, x2_new) > func(x1, x2) + epsilon * (grad_x1 * d_x1 + grad_x2 * d_x2)) {
            lambda *= 0.5;
            x1_new = x1 + lambda * d_x1;
            x2_new = x2 + lambda * d_x2;
        }

        // Обновляем значения
        x1 = x1_new;
        x2 = x2_new;

        k++; // Увеличиваем счетчик итераций
    }
}
double goldenSectionSearch(const double* x, const double* d);

// Метод наискорейшего градиентного спуска с подбором шага через метод золотого сечения
int steepest_descent(double& x1, double& x2) {
    int k = 0; // Инициализируем счётчик итераций

    // Основной итерационный цикл, продолжаем, пока k меньше максимального числа итераций M
    while (k < M) {
        // 1. Вычисляем градиент в текущей точке (x1, x2)
        double grad_x1 = func_proizv_x1(x1, x2); // Первая производная по x1
        double grad_x2 = func_proizv_x2(x1, x2); // Первая производная по x2

        // 2. Проверяем условие сходимости: если норма градиента меньше epsilon, то минимум найден
        if (sqrt(grad_x1 * grad_x1 + grad_x2 * grad_x2) < epsilon) {
            cout << "Метод наискорейшего градиентного спуска завершен: x* = ("
                << fixed << setprecision(2) << x1 << ", " << x2 << ")"
                << "\t f(x1,x2)= " << func(x1, x2) << endl;
            cout << "Количество итераций: " << k << endl;
            return k;
        }

        // 3. Формируем массив для текущей точки, чтобы передать его в функцию золотого сечения
        double x_arr[2] = { x1, x2 };
        // Вычисляем направление спуска d = -∇f(x)
        double d[2] = { -grad_x1, -grad_x2 };

        // 4. Вызываем метод золотого сечения для поиска оптимального шага alpha вдоль направления d
        double alpha = goldenSectionSearch(x_arr, d);

        // 5. Обновляем текущую точку по правилу: x^(k+1) = x^(k) + alpha * d
        x1 = x1 + alpha * d[0];
        x2 = x2 + alpha * d[1];

        // 6. Увеличиваем счётчик итераций
        k++;
    }

    // Если цикл завершился из-за достижения максимального числа итераций, выводим сообщение
    cout << "Достигнуто максимальное число итераций в методе наискорейшего градиентного спуска." << endl;
    return -1; // Возвращаем -1, если алгоритм не сошелся за M итераций
}

// Определение целевой функции для метода золотого сечения
double function_gold(const double* x) {
    double x1 = x[0];
    double x2 = x[1];
    return (x1 - 2 * x2) * (x1 - 2 * x2) + (x2 - 3) * (x2 - 3);
}

// Определение градиента целевой функции
void gradient_gold(const double* x, double* result) {
    double x1 = x[0];
    double x2 = x[1];
    result[0] = 2 * (x1 - 2 * x2);         // Производная по x1
    result[1] = -4 * (x1 - 2 * x2) + 2 * (x2 - 3); // Производная по x2
}

// Функция для вычисления нормы вектора
double norm_gold(const double* x, int n) {
    double sum = 0.0;
    for (int i = 0; i < n; ++i) {
        sum += x[i] * x[i];
    }
    return std::sqrt(sum);
}

// Реализация метода золотого сечения для поиска оптимального шага lambda
double goldenSectionSearch(const double* x, const double* d) {
    double a = 0.0;
    double b = 1.0; // Начальный интервал для поиска lambda
    double tol = 1e-8; // Точность поиска (увеличена)
    const double goldenRatio = (1 + std::sqrt(5)) / 2;

    double c = b - (b - a) / goldenRatio;
    double d_val = a + (b - a) / goldenRatio;

    while (std::abs(b - a) > tol) {
        double x_c[2], x_d[2];
        x_c[0] = x[0] + c * d[0];
        x_c[1] = x[1] + c * d[1];
        x_d[0] = x[0] + d_val * d[0];
        x_d[1] = x[1] + d_val * d[1];

        if (function_gold(x_c) < function_gold(x_d)) {
            b = d_val;
        }
        else {
            a = c;
        }

        c = b - (b - a) / goldenRatio;
        d_val = a + (b - a) / goldenRatio;
    }

    return (a + b) / 2.0; // Возвращаем приближенное значение lambda
}

// Метод сопряженных градиентов с золотым сечением
int conjugate_gradient_gold(double& x1, double& x2) {
    int k = 0; // Номер итерации
    int maxIterations = M; // Максимальное количество итераций (M определено ранее)
    double epsilon_gold = epsilon; // Критерий сходимости (epsilon определено ранее)
    const int dimension = 2; // Размерность задачи

    double x[dimension]; // Текущее решение x^k
    x[0] = x1; // Начальное предположение для x1
    x[1] = x2; // Начальное предположение для x2

    double d[dimension]; // Направление поиска d_k
    double grad[dimension]; // Градиент функции в текущей точке
    double grad_prev[dimension]; // Градиент на предыдущей итерации (нужен для omega)
    double d_prev[dimension]; // Предыдущее направление (нужно для omega)

    // Инициализация градиента и направления поиска
    gradient_gold(x, grad);
    for (int i = 0; i < dimension; ++i) {
        d[i] = -grad[i]; // Начальное направление поиска
        d_prev[i] = d[i]; // Инициализируем d_prev для первой итерации
        grad_prev[i] = grad[i];
    }

    double x_new[dimension]; // Следующее решение x^{k+1}

    while (true) {
        // 2. Вычислить lambda_k с использованием метода золотого сечения
        double lambda = goldenSectionSearch(x, d);

        // 3. Вычислить x^{k+1} = x^k + lambda_k * d_k
        for (int i = 0; i < dimension; ++i) {
            x_new[i] = x[i] + lambda * d[i];
        }

        // Вычисление нового градиента
        double grad_new[dimension];
        gradient_gold(x_new, grad_new);

        // 5. Проверить критерий остановки ||d_k|| < epsilon или k + 1 > M
        double norm_d = norm_gold(d, dimension);
        if (norm_d < epsilon_gold || k + 1 > maxIterations) {
            cout << "Метод сопряженных градиентов с золотым сечением завершен: x* = ("
                << fixed << setprecision(6) << x_new[0] << ", "
                << fixed << setprecision(6) << x_new[1] << ")" << "\t"
                << "f(x1,x2)= " << func(x_new[0], x_new[1]) << endl;
            cout << "Количество итераций: " << k << endl;

            x1 = x_new[0];
            x2 = x_new[1];
            return k;
        }

        // Обновление номера шага
        k++;

        // 4. Вычислить новое направление поиска d_k
        double omega = 0.0;
        double grad_norm_sq = 0.0;
        double grad_prev_norm_sq = 0.0;

        for (int i = 0; i < dimension; ++i) {
            grad_norm_sq += grad_new[i] * grad_new[i];
            grad_prev_norm_sq += grad[i] * grad[i];
        }

        omega = grad_norm_sq / grad_prev_norm_sq;

        // Сохраняем предыдущее направление
        for (int i = 0; i < dimension; ++i) {
            d_prev[i] = d[i];
        }

        // Вычисляем новое направление
        for (int i = 0; i < dimension; ++i) {
            d[i] = -grad_new[i] + omega * d_prev[i];
        }

        // Обновить x и градиент для следующей итерации
        for (int i = 0; i < dimension; ++i) {
            x[i] = x_new[i];
            grad[i] = grad_new[i];
        }
    }
    cout << "Достигнуто максимальное число итераций в методе сопряженных градиентов с золотым сечением." << endl;
    return -1;
}


int main() {
    setlocale(LC_ALL, "rus");
    // Начальные значения
    double x1 = 0.0;
    double x2 = 0.0;
    double x1_copy = x1;
    double x2_copy = x2;

    cout << "Начальные значения: x1 = " << x1 << ", x2 = " << x2 << endl;

    // Запуск метода Ньютона-Рафсона
    cout << "\nМетод Ньютона-Рафсона:" << endl;
    newton_raphson(x1, x2);

    // Сбрасываем значения для следующих методов
    x1 = x1_copy;
    x2 = x2_copy;

    // Запуск метода Марквардта
    cout << "\nМетод Марквардта:" << endl;
    Markvarda(x1, x2);

    // Сбрасываем значения для следующих методов
    x1 = x1_copy;
    x2 = x2_copy;

    // Запуск метода наискорейшего градиентного спуска (исправленный)
    cout << "\nМетод наискорейшего градиентного спуска:" << endl;
    steepest_descent(x1, x2);

    // Сбрасываем значения для следующих методов
    x1 = x1_copy;
    x2 = x2_copy;

    //// Запуск метода сопряженных градиентов
    //cout << "\nМетод сопряженных градиентов:" << endl;
    //conjugate_gradient(x1, x2);

    // Сбрасываем значения для следующих методов
    x1 = x1_copy;
    x2 = x2_copy;

    // Запуск метода сопряженных градиентов с золотым сечением
    cout << "\nМетод сопряженных градиентов с золотым сечением:" << endl;
    conjugate_gradient_gold(x1, x2);

    return 0;
}
